Training fold 1
In epoch:000|batch:0000, train_loss:0.806595, train_ap:0.3421, train_acc:0.4062, train_auc:0.5565
In epoch:000|batch:0010, train_loss:1.505326, train_ap:0.2742, train_acc:0.8125, train_auc:0.5901
In epoch:000|batch:0020, train_loss:1.101072, train_ap:0.2509, train_acc:0.8750, train_auc:0.5268
In epoch:000|batch:0030, train_loss:0.908457, train_ap:0.2417, train_acc:0.7656, train_auc:0.4490
In epoch:000|batch:0040, train_loss:0.805153, train_ap:0.1635, train_acc:0.8594, train_auc:0.6391
In epoch:000|batch:0050, train_loss:0.742986, train_ap:0.1570, train_acc:0.9219, train_auc:0.5831
In epoch:000|batch:0060, train_loss:0.692739, train_ap:0.1407, train_acc:0.8906, train_auc:0.5840
In epoch:000|batch:0070, train_loss:0.662199, train_ap:0.2107, train_acc:0.8594, train_auc:0.4303
In epoch:000|batch:0080, train_loss:0.633335, train_ap:0.4209, train_acc:0.7812, train_auc:0.6714
In epoch:000|batch:0090, train_loss:0.611034, train_ap:0.1129, train_acc:0.8906, train_auc:0.4787
In epoch:000|batch:0100, train_loss:0.592036, train_ap:0.1701, train_acc:0.8438, train_auc:0.4889
In epoch:000|batch:0110, train_loss:0.575575, train_ap:0.1740, train_acc:0.8750, train_auc:0.6049
In epoch:000|batch:0120, train_loss:0.562650, train_ap:0.3198, train_acc:0.7812, train_auc:0.6457
In epoch:000|batch:0130, train_loss:0.551610, train_ap:0.4106, train_acc:0.8906, train_auc:0.6591
In epoch:000|batch:0140, train_loss:0.538799, train_ap:0.1130, train_acc:0.8750, train_auc:0.3504
In epoch:000|batch:0150, train_loss:0.529390, train_ap:0.2767, train_acc:0.9531, train_auc:0.8197
In epoch:000|batch:0160, train_loss:0.522364, train_ap:0.1653, train_acc:0.9062, train_auc:0.4828
In epoch:000|batch:0170, train_loss:0.516671, train_ap:0.1942, train_acc:0.8906, train_auc:0.5263
In epoch:000|batch:0180, train_loss:0.510702, train_ap:0.1636, train_acc:0.8594, train_auc:0.5253
In epoch:000|batch:0190, train_loss:0.506148, train_ap:0.2874, train_acc:0.8594, train_auc:0.4828
In epoch:000|batch:0200, train_loss:0.499623, train_ap:0.0545, train_acc:0.9531, train_auc:0.3880
In epoch:000|batch:0210, train_loss:0.500050, train_ap:0.2811, train_acc:0.8750, train_auc:0.7054
In epoch:000|batch:0000, val_loss:0.006591, val_ap:0.2089, val_acc:0.8594, val_auc:0.5556
In epoch:000|batch:0010, val_loss:0.007033, val_ap:0.2285, val_acc:0.8750, val_auc:0.5871
In epoch:000|batch:0020, val_loss:0.006916, val_ap:0.0677, val_acc:0.9219, val_auc:0.3424
In epoch:000|batch:0030, val_loss:0.006978, val_ap:0.1460, val_acc:0.8438, val_auc:0.4278
In epoch:000|batch:0040, val_loss:0.006837, val_ap:0.0828, val_acc:0.9375, val_auc:0.4271
In epoch:000|batch:0050, val_loss:0.006725, val_ap:0.1279, val_acc:0.9375, val_auc:0.5729
In epoch:000|batch:0060, val_loss:0.006714, val_ap:0.1006, val_acc:0.9375, val_auc:0.5542
In epoch:000|batch:0070, val_loss:0.006687, val_ap:0.1185, val_acc:0.8906, val_auc:0.4712
In epoch:000|batch:0080, val_loss:0.006655, val_ap:0.2872, val_acc:0.8438, val_auc:0.5741
In epoch:000|batch:0090, val_loss:0.006677, val_ap:0.0763, val_acc:0.9688, val_auc:0.6371
In epoch:000|batch:0100, val_loss:0.006717, val_ap:0.3722, val_acc:0.8438, val_auc:0.6889
In epoch:000|batch:0110, val_loss:0.006705, val_ap:0.3315, val_acc:0.8281, val_auc:0.4966
In epoch:000|batch:0120, val_loss:0.006674, val_ap:0.1030, val_acc:0.9062, val_auc:0.5072
In epoch:000|batch:0130, val_loss:0.006695, val_ap:0.1884, val_acc:0.7812, val_auc:0.3500
In epoch:000|batch:0140, val_loss:0.006656, val_ap:0.1383, val_acc:0.9219, val_auc:0.5797
In epoch:000|batch:0150, val_loss:0.006671, val_ap:0.3328, val_acc:0.7969, val_auc:0.5596
In epoch:000|batch:0160, val_loss:0.006630, val_ap:0.2669, val_acc:0.9062, val_auc:0.5718
In epoch:000|batch:0170, val_loss:0.006639, val_ap:0.1324, val_acc:0.8594, val_auc:0.4404
In epoch:000|batch:0180, val_loss:0.006633, val_ap:0.1338, val_acc:0.9062, val_auc:0.5374
In epoch:000|batch:0190, val_loss:0.006636, val_ap:0.0724, val_acc:0.9219, val_auc:0.3729
In epoch:000|batch:0200, val_loss:0.006674, val_ap:0.3816, val_acc:0.7812, val_auc:0.5929
In epoch:000|batch:0210, val_loss:0.006685, val_ap:0.1333, val_acc:0.8594, val_auc:0.4242
Best val_loss is: 0.0067072
In test batch:0000
In test batch:0010
In test batch:0020
In test batch:0030
In test batch:0040
In test batch:0050
In test batch:0060
In test batch:0070
In test batch:0080
In test batch:0090
In test batch:0100
In test batch:0110
In test batch:0120
In test batch:0130
In test batch:0140
In test batch:0150
In test batch:0160
In test batch:0170
In test batch:0180
In test batch:0190
In test batch:0200
In test batch:0210
In test batch:0220
In test batch:0230
In test batch:0240
In test batch:0250
In test batch:0260
In test batch:0270
In test batch:0280
Training fold 2
In epoch:000|batch:0000, train_loss:0.726366, train_ap:0.1934, train_acc:0.4219, train_auc:0.5848
In epoch:000|batch:0010, train_loss:1.062363, train_ap:0.2578, train_acc:0.8594, train_auc:0.4788
In epoch:000|batch:0020, train_loss:0.814667, train_ap:0.1695, train_acc:0.8438, train_auc:0.5647
In epoch:000|batch:0030, train_loss:0.728699, train_ap:0.2297, train_acc:0.8281, train_auc:0.5986
In epoch:000|batch:0040, train_loss:0.667300, train_ap:0.1206, train_acc:0.8438, train_auc:0.4196
In epoch:000|batch:0050, train_loss:0.622152, train_ap:0.1503, train_acc:0.9219, train_auc:0.6203
In epoch:000|batch:0060, train_loss:0.584680, train_ap:0.2259, train_acc:0.8906, train_auc:0.3885
In epoch:000|batch:0070, train_loss:0.566306, train_ap:0.2144, train_acc:0.8125, train_auc:0.4391
In epoch:000|batch:0080, train_loss:0.545258, train_ap:0.1005, train_acc:0.8906, train_auc:0.3835
In epoch:000|batch:0090, train_loss:0.531440, train_ap:0.1179, train_acc:0.8906, train_auc:0.2607
In epoch:000|batch:0100, train_loss:0.524686, train_ap:0.2370, train_acc:0.7656, train_auc:0.4626
In epoch:000|batch:0110, train_loss:0.514987, train_ap:0.0872, train_acc:0.9062, train_auc:0.4080
In epoch:000|batch:0120, train_loss:0.504043, train_ap:0.1363, train_acc:0.8281, train_auc:0.3345
In epoch:000|batch:0130, train_loss:0.497672, train_ap:0.1892, train_acc:0.8750, train_auc:0.5670
In epoch:000|batch:0140, train_loss:0.492052, train_ap:0.2360, train_acc:0.8438, train_auc:0.5685
In epoch:000|batch:0150, train_loss:0.487386, train_ap:0.3170, train_acc:0.7656, train_auc:0.5211
In epoch:000|batch:0160, train_loss:0.482384, train_ap:0.0894, train_acc:0.9375, train_auc:0.5833
In epoch:000|batch:0170, train_loss:0.479190, train_ap:0.1564, train_acc:0.8125, train_auc:0.3862
In epoch:000|batch:0180, train_loss:0.475420, train_ap:0.1079, train_acc:0.9219, train_auc:0.4814
In epoch:000|batch:0190, train_loss:0.472551, train_ap:0.1211, train_acc:0.8906, train_auc:0.4762
In epoch:000|batch:0200, train_loss:0.471496, train_ap:0.2210, train_acc:0.8438, train_auc:0.5333
In epoch:000|batch:0210, train_loss:0.468846, train_ap:0.4521, train_acc:0.8594, train_auc:0.7273
In epoch:000|batch:0000, val_loss:0.009172, val_ap:0.1603, val_acc:0.8438, val_auc:0.3222
In epoch:000|batch:0010, val_loss:0.007592, val_ap:0.1555, val_acc:0.8594, val_auc:0.5091
In epoch:000|batch:0020, val_loss:0.007260, val_ap:0.2484, val_acc:0.8438, val_auc:0.6500
In epoch:000|batch:0030, val_loss:0.006966, val_ap:0.1201, val_acc:0.9062, val_auc:0.5259
In epoch:000|batch:0040, val_loss:0.006673, val_ap:0.1178, val_acc:0.8906, val_auc:0.4912
In epoch:000|batch:0050, val_loss:0.006803, val_ap:0.1041, val_acc:0.8750, val_auc:0.3013
In epoch:000|batch:0060, val_loss:0.006947, val_ap:0.1627, val_acc:0.9062, val_auc:0.5833
In epoch:000|batch:0070, val_loss:0.006870, val_ap:0.2210, val_acc:0.8906, val_auc:0.6992
In epoch:000|batch:0080, val_loss:0.006978, val_ap:0.3346, val_acc:0.8125, val_auc:0.6410
In epoch:000|batch:0090, val_loss:0.007052, val_ap:0.2658, val_acc:0.7969, val_auc:0.5656
In epoch:000|batch:0100, val_loss:0.007040, val_ap:0.0759, val_acc:0.9062, val_auc:0.3161
In epoch:000|batch:0110, val_loss:0.007011, val_ap:0.1553, val_acc:0.8594, val_auc:0.4848
In epoch:000|batch:0120, val_loss:0.006958, val_ap:0.1470, val_acc:0.8750, val_auc:0.5067
In epoch:000|batch:0130, val_loss:0.007065, val_ap:0.1699, val_acc:0.8125, val_auc:0.3814
In epoch:000|batch:0140, val_loss:0.007029, val_ap:0.1562, val_acc:0.8281, val_auc:0.4305
In epoch:000|batch:0150, val_loss:0.007000, val_ap:0.2825, val_acc:0.7812, val_auc:0.5429
In epoch:000|batch:0160, val_loss:0.006996, val_ap:0.1628, val_acc:0.8750, val_auc:0.5156
In epoch:000|batch:0170, val_loss:0.007034, val_ap:0.1740, val_acc:0.8438, val_auc:0.5500
In epoch:000|batch:0180, val_loss:0.007055, val_ap:0.1129, val_acc:0.9062, val_auc:0.5287
In epoch:000|batch:0190, val_loss:0.007057, val_ap:0.1344, val_acc:0.8438, val_auc:0.3907
In epoch:000|batch:0200, val_loss:0.007066, val_ap:0.1986, val_acc:0.7969, val_auc:0.2919
In epoch:000|batch:0210, val_loss:0.007031, val_ap:0.1497, val_acc:0.8906, val_auc:0.4637
Best val_loss is: 0.0070385
In test batch:0000
In test batch:0010
In test batch:0020
In test batch:0030
In test batch:0040
In test batch:0050
In test batch:0060
In test batch:0070
In test batch:0080
In test batch:0090
In test batch:0100
In test batch:0110
In test batch:0120
In test batch:0130
In test batch:0140
In test batch:0150
In test batch:0160
In test batch:0170
In test batch:0180
In test batch:0190
In test batch:0200
In test batch:0210
In test batch:0220
In test batch:0230
In test batch:0240
In test batch:0250
In test batch:0260
In test batch:0270
In test batch:0280
NN out of fold AP is: 0.14512878709670646
test AUC:0.4847139509620144
test f1:0.4608277358988649
test AP:0.13718643428295782
