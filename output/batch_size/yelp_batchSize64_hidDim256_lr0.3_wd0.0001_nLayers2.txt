Training fold 1
In epoch:000|batch:0000, train_loss:0.615956, train_ap:0.1959, train_acc:0.6328, train_auc:0.5201
In epoch:000|batch:0010, train_loss:1.624178, train_ap:0.1353, train_acc:0.8906, train_auc:0.5927
In epoch:000|batch:0020, train_loss:1.169945, train_ap:0.1221, train_acc:0.8750, train_auc:0.4843
In epoch:000|batch:0030, train_loss:0.943341, train_ap:0.1038, train_acc:0.8516, train_auc:0.4277
In epoch:000|batch:0040, train_loss:0.815000, train_ap:0.1792, train_acc:0.8359, train_auc:0.4993
In epoch:000|batch:0050, train_loss:0.739729, train_ap:0.1783, train_acc:0.8594, train_auc:0.5556
In epoch:000|batch:0060, train_loss:0.688832, train_ap:0.1415, train_acc:0.8516, train_auc:0.4201
In epoch:000|batch:0070, train_loss:0.651292, train_ap:0.1197, train_acc:0.8750, train_auc:0.4715
In epoch:000|batch:0080, train_loss:0.621523, train_ap:0.1753, train_acc:0.8438, train_auc:0.5245
In epoch:000|batch:0090, train_loss:0.598549, train_ap:0.1815, train_acc:0.8516, train_auc:0.5567
In epoch:000|batch:0100, train_loss:0.580497, train_ap:0.1377, train_acc:0.8828, train_auc:0.5558
In epoch:000|batch:0000, val_loss:0.003559, val_ap:0.2040, val_acc:0.8359, val_auc:0.5283
In epoch:000|batch:0010, val_loss:0.003302, val_ap:0.1245, val_acc:0.8906, val_auc:0.5263
In epoch:000|batch:0020, val_loss:0.003302, val_ap:0.2566, val_acc:0.8672, val_auc:0.5962
In epoch:000|batch:0030, val_loss:0.003365, val_ap:0.2190, val_acc:0.8047, val_auc:0.5134
In epoch:000|batch:0040, val_loss:0.003374, val_ap:0.1845, val_acc:0.8516, val_auc:0.5635
In epoch:000|batch:0050, val_loss:0.003328, val_ap:0.2404, val_acc:0.8594, val_auc:0.5167
In epoch:000|batch:0060, val_loss:0.003279, val_ap:0.1654, val_acc:0.8906, val_auc:0.5921
In epoch:000|batch:0070, val_loss:0.003277, val_ap:0.1111, val_acc:0.8906, val_auc:0.3835
In epoch:000|batch:0080, val_loss:0.003267, val_ap:0.1464, val_acc:0.8281, val_auc:0.4005
In epoch:000|batch:0090, val_loss:0.003257, val_ap:0.2773, val_acc:0.8203, val_auc:0.5741
In epoch:000|batch:0100, val_loss:0.003258, val_ap:0.2120, val_acc:0.8359, val_auc:0.5042
Best val_loss is: 0.0032722
In test batch:0000
In test batch:0010
In test batch:0020
In test batch:0030
In test batch:0040
In test batch:0050
In test batch:0060
In test batch:0070
In test batch:0080
In test batch:0090
In test batch:0100
In test batch:0110
In test batch:0120
In test batch:0130
In test batch:0140
Training fold 2
In epoch:000|batch:0000, train_loss:0.719971, train_ap:0.1940, train_acc:0.4844, train_auc:0.4949
In epoch:000|batch:0010, train_loss:1.711986, train_ap:0.2347, train_acc:0.8359, train_auc:0.6689
In epoch:000|batch:0020, train_loss:1.210281, train_ap:0.1700, train_acc:0.7891, train_auc:0.4214
In epoch:000|batch:0030, train_loss:0.958287, train_ap:0.1459, train_acc:0.8750, train_auc:0.5703
In epoch:000|batch:0040, train_loss:0.837251, train_ap:0.1151, train_acc:0.8438, train_auc:0.3222
In epoch:000|batch:0050, train_loss:0.755090, train_ap:0.1665, train_acc:0.8750, train_auc:0.5413
In epoch:000|batch:0060, train_loss:0.703813, train_ap:0.2149, train_acc:0.8125, train_auc:0.4451
In epoch:000|batch:0070, train_loss:0.666050, train_ap:0.1934, train_acc:0.8359, train_auc:0.5581
In epoch:000|batch:0080, train_loss:0.632704, train_ap:0.1163, train_acc:0.9141, train_auc:0.5315
In epoch:000|batch:0090, train_loss:0.606508, train_ap:0.1547, train_acc:0.9062, train_auc:0.3606
In epoch:000|batch:0100, train_loss:0.585866, train_ap:0.0927, train_acc:0.8906, train_auc:0.3979
In epoch:000|batch:0000, val_loss:0.002949, val_ap:0.1928, val_acc:0.8750, val_auc:0.5329
In epoch:000|batch:0010, val_loss:0.003042, val_ap:0.1047, val_acc:0.8672, val_auc:0.3662
In epoch:000|batch:0020, val_loss:0.003147, val_ap:0.2109, val_acc:0.8672, val_auc:0.5681
In epoch:000|batch:0030, val_loss:0.003137, val_ap:0.1837, val_acc:0.8125, val_auc:0.4505
In epoch:000|batch:0040, val_loss:0.003171, val_ap:0.1650, val_acc:0.8516, val_auc:0.4102
In epoch:000|batch:0050, val_loss:0.003199, val_ap:0.3089, val_acc:0.7734, val_auc:0.5859
In epoch:000|batch:0060, val_loss:0.003177, val_ap:0.1297, val_acc:0.8906, val_auc:0.4596
In epoch:000|batch:0070, val_loss:0.003219, val_ap:0.1195, val_acc:0.8516, val_auc:0.3766
In epoch:000|batch:0080, val_loss:0.003229, val_ap:0.0892, val_acc:0.8906, val_auc:0.3904
In epoch:000|batch:0090, val_loss:0.003229, val_ap:0.1938, val_acc:0.8828, val_auc:0.5726
In epoch:000|batch:0100, val_loss:0.003244, val_ap:0.1994, val_acc:0.8594, val_auc:0.5141
Best val_loss is: 0.0032503
In test batch:0000
In test batch:0010
In test batch:0020
In test batch:0030
In test batch:0040
In test batch:0050
In test batch:0060
In test batch:0070
In test batch:0080
In test batch:0090
In test batch:0100
In test batch:0110
In test batch:0120
In test batch:0130
In test batch:0140
NN out of fold AP is: 0.14088697597760805
test AUC:0.4674403092492363
test f1:0.4608277358988649
test AP:0.13263359460055354
