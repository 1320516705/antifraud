Training fold 1
In epoch:000|batch:0000, train_loss:0.629476, train_ap:0.1742, train_acc:0.6641, train_auc:0.5077
In epoch:000|batch:0010, train_loss:0.530511, train_ap:0.1946, train_acc:0.8438, train_auc:0.5866
In epoch:000|batch:0020, train_loss:0.483842, train_ap:0.2488, train_acc:0.8203, train_auc:0.5477
In epoch:000|batch:0030, train_loss:0.471002, train_ap:0.2199, train_acc:0.8047, train_auc:0.4462
In epoch:000|batch:0040, train_loss:0.459878, train_ap:0.1402, train_acc:0.8906, train_auc:0.4768
In epoch:000|batch:0050, train_loss:0.447589, train_ap:0.1320, train_acc:0.9062, train_auc:0.5575
In epoch:000|batch:0060, train_loss:0.442507, train_ap:0.1962, train_acc:0.8516, train_auc:0.5717
In epoch:000|batch:0070, train_loss:0.440926, train_ap:0.1348, train_acc:0.8672, train_auc:0.4849
In epoch:000|batch:0080, train_loss:0.434778, train_ap:0.2026, train_acc:0.8672, train_auc:0.5215
In epoch:000|batch:0090, train_loss:0.434393, train_ap:0.2004, train_acc:0.8828, train_auc:0.5428
In epoch:000|batch:0100, train_loss:0.434467, train_ap:0.1013, train_acc:0.8750, train_auc:0.3923
In epoch:000|batch:0000, val_loss:0.002523, val_ap:0.2800, val_acc:0.9062, val_auc:0.6925
In epoch:000|batch:0010, val_loss:0.003145, val_ap:0.1809, val_acc:0.8828, val_auc:0.4950
In epoch:000|batch:0020, val_loss:0.003187, val_ap:0.1604, val_acc:0.8594, val_auc:0.4601
In epoch:000|batch:0030, val_loss:0.003204, val_ap:0.2120, val_acc:0.9062, val_auc:0.6365
In epoch:000|batch:0040, val_loss:0.003234, val_ap:0.3191, val_acc:0.8594, val_auc:0.5778
In epoch:000|batch:0050, val_loss:0.003198, val_ap:0.2116, val_acc:0.8906, val_auc:0.6165
In epoch:000|batch:0060, val_loss:0.003197, val_ap:0.0921, val_acc:0.9141, val_auc:0.3419
In epoch:000|batch:0070, val_loss:0.003233, val_ap:0.2095, val_acc:0.8359, val_auc:0.5029
In epoch:000|batch:0080, val_loss:0.003210, val_ap:0.1321, val_acc:0.8828, val_auc:0.4283
In epoch:000|batch:0090, val_loss:0.003205, val_ap:0.1643, val_acc:0.8594, val_auc:0.5298
In epoch:000|batch:0100, val_loss:0.003214, val_ap:0.1700, val_acc:0.8438, val_auc:0.5417
Best val_loss is: 0.0032285
In test batch:0000
In test batch:0010
In test batch:0020
In test batch:0030
In test batch:0040
In test batch:0050
In test batch:0060
In test batch:0070
In test batch:0080
In test batch:0090
In test batch:0100
In test batch:0110
In test batch:0120
In test batch:0130
In test batch:0140
Training fold 2
In epoch:000|batch:0000, train_loss:0.675530, train_ap:0.3222, train_acc:0.5703, train_auc:0.6437
In epoch:000|batch:0010, train_loss:0.537254, train_ap:0.2923, train_acc:0.8828, train_auc:0.7740
In epoch:000|batch:0020, train_loss:0.484141, train_ap:0.2622, train_acc:0.7969, train_auc:0.6944
In epoch:000|batch:0030, train_loss:0.471962, train_ap:0.2172, train_acc:0.8359, train_auc:0.6195
In epoch:000|batch:0040, train_loss:0.462184, train_ap:0.1821, train_acc:0.8594, train_auc:0.5525
In epoch:000|batch:0050, train_loss:0.451523, train_ap:0.2783, train_acc:0.8359, train_auc:0.7485
In epoch:000|batch:0060, train_loss:0.443773, train_ap:0.5083, train_acc:0.8672, train_auc:0.7854
In epoch:000|batch:0070, train_loss:0.433063, train_ap:0.1536, train_acc:0.8438, train_auc:0.5497
In epoch:000|batch:0080, train_loss:0.424895, train_ap:0.4390, train_acc:0.8281, train_auc:0.7375
In epoch:000|batch:0090, train_loss:0.419846, train_ap:0.2367, train_acc:0.8359, train_auc:0.6205
In epoch:000|batch:0100, train_loss:0.414986, train_ap:0.3640, train_acc:0.8906, train_auc:0.7179
In epoch:000|batch:0000, val_loss:0.002948, val_ap:0.4122, val_acc:0.8516, val_auc:0.7197
In epoch:000|batch:0010, val_loss:0.002732, val_ap:0.4455, val_acc:0.8281, val_auc:0.7935
In epoch:000|batch:0020, val_loss:0.002866, val_ap:0.2376, val_acc:0.8203, val_auc:0.6866
In epoch:000|batch:0030, val_loss:0.002775, val_ap:0.5166, val_acc:0.8281, val_auc:0.8125
In epoch:000|batch:0040, val_loss:0.002789, val_ap:0.1976, val_acc:0.8906, val_auc:0.6386
In epoch:000|batch:0050, val_loss:0.002796, val_ap:0.5356, val_acc:0.8828, val_auc:0.8294
In epoch:000|batch:0060, val_loss:0.002772, val_ap:0.6110, val_acc:0.9297, val_auc:0.8863
In epoch:000|batch:0070, val_loss:0.002771, val_ap:0.4038, val_acc:0.8516, val_auc:0.7785
In epoch:000|batch:0080, val_loss:0.002783, val_ap:0.4624, val_acc:0.8203, val_auc:0.7993
In epoch:000|batch:0090, val_loss:0.002800, val_ap:0.3591, val_acc:0.8203, val_auc:0.7954
In epoch:000|batch:0100, val_loss:0.002785, val_ap:0.4487, val_acc:0.8828, val_auc:0.7717
Best val_loss is: 0.0028120
In test batch:0000
In test batch:0010
In test batch:0020
In test batch:0030
In test batch:0040
In test batch:0050
In test batch:0060
In test batch:0070
In test batch:0080
In test batch:0090
In test batch:0100
In test batch:0110
In test batch:0120
In test batch:0130
In test batch:0140
NN out of fold AP is: 0.29780705714717526
test AUC:0.7711548121356453
test f1:0.5951637973622661
test AP:0.3915738625974138
